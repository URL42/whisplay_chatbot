const fs = require("fs");
const path = require("path");
const { display, extractEmojis, onButtonPressed } = require("./device/display");
const {
  recordAudio,
  StreamResponser,
  stopRecording,
} = require("./device/audio");
const {
  chatWithLLMStream,
  recognizeAudio,
  ttsProcessor,
} = require("./cloud-api/server");
const { noop } = require("lodash");

const {
  partial,
  endPartial,
  getPlayEndPromise,
  stop: stopPlaying,
} = new StreamResponser(
  ttsProcessor,
  (sentences) => {
    const fullText = sentences.join("");
    display({
      status: "answering",
      emoji: extractEmojis(fullText) || "ðŸ˜Š",
      text: fullText,
      RGB: "#0000ff",
    });
  },
  (text) => {
    console.log("å®Œæ•´å›žç­”:", text);
    display({
      text,
    });
  }
);

// flowStatus "sleep", "listen", "asr", "answer"
let recordFilePath = "";
let asrText = "";

// if data folder not exist, create it
const dataDir = path.join(__dirname, "data");
if (!fs.existsSync(dataDir)) {
  fs.mkdirSync(dataDir);
  console.log("åˆ›å»ºæ•°æ®æ–‡ä»¶å¤¹:", dataDir);
} else {
  console.log("æ•°æ®æ–‡ä»¶å¤¹å·²å­˜åœ¨:", dataDir);
}

let currentStatus = "sleep";

const executeFlow = async (flowStatus, isButtonClick) => {
  if (flowStatus === currentStatus && !isButtonClick) return
  switch (flowStatus) {
    case "sleep":
      currentStatus = "sleep";
      console.log("å¾…æœº");
      display({
        status: "idle",
        emoji: "ðŸ˜´",
        RGB: "#000055",
        text: "Press the button to start",
      });
      onButtonPressed(() => {
        executeFlow("listen", true);
      });
      break;
    case "listen":
      currentStatus = "listen";
      console.log("è†å¬ä¸­...");
      display({ status: "listening", emoji: "ðŸ˜", RGB: "#00ff00" });
      recordFilePath = `${dataDir}/user-${Date.now()}.mp3`;
      recordAudio(recordFilePath, 60)
        .then(() => {
          executeFlow("asr");
        })
        .catch((err) => {
          console.error("å½•éŸ³é”™è¯¯:", err);
          executeFlow("listen");
        });
      onButtonPressed(() => {
        stopRecording();
        executeFlow("sleep", true);
      });
      break;
    case "asr":
      currentStatus = "asr";
      console.log("è¯†åˆ«ä¸­...");
      asrText = "";
      display({ status: "recognizing", emoji: "ðŸ¤”", text: "" });
      let userStop = noop;
      Promise.race([
        recognizeAudio(recordFilePath).then((text) => {
          asrText = text;
          display({ text });
          if (text) {
            executeFlow("answer");
          } else {
            console.log("è¯†åˆ«ç»“æžœä¸ºç©º, è¯·ç»§ç»­è¯´");
            executeFlow("listen");
          }
        }),
        new Promise((resolve) => {
          userStop = resolve;
        }),
      ]).then((result) => {
        if (result === "[UserStop]") {
          executeFlow("listen", true);
        }
      });
      onButtonPressed(() => {
        userStop("[UserStop]");
      });
      break;
    case "answer":
      currentStatus = "answer";
      console.log("å›žç­”ä¸­...");
      let userStopAnser = noop;
      const answerPromise = Promise.all([
        chatWithLLMStream([{
          role: 'user',
          content: asrText,
        }], partial, endPartial),
        getPlayEndPromise(),
      ]);
      Promise.race([
        answerPromise,
        new Promise((resolve) => {
          userStopAnser = resolve;
        }),
      ]).then((res) => {
        executeFlow("listen", res === "[UserStop]");
      });
      onButtonPressed(() => {
        userStopAnser("[UserStop]");
        stopPlaying();
      });
      break;
  }
};

(async () => {
  executeFlow("sleep");
})();
