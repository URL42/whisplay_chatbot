# specify the ASR、LLM and TTS server to use
# options: TENCENT, VOLCENGINE, OPENAI
ASR_SERVER=TENCENT 

# options: VOLCENGINE, OPENAI, OLLAMA
LLM_SERVER=VOLCENGINE 

# options: TENCENT, VOLCENGINE, OPENAI
TTS_SERVER=VOLCENGINE

# specify the chat history reset time in seconds, default is 5 minutes (300 seconds)
# CHAT_HISTORY_RESET_TIME=300

# enable or disable thinking of LLM
# if you are using ollama as LLM server, please confirm that the model supports thinking before enabling this option
# otherwise the ollama will return a 400 error
ENABLE_THINKING=true


## Tencent Cloud ASR and TTS
# if you are using tencent cloud as ASR or TTS server, please set the following environment variables
TENCENT_SECRET_ID=YourSecretId
TENCENT_SECRET_KEY=YourSecretKey
# endpoint is optional, default is asr.tencentcloudapi.com for ASR and tts.tencentcloudapi.com for TTS
# TENCENT_ASR_ENDPOINT=asr.tencentcloudapi.com
# TENCENT_TTS_ENDPOINT=tts.tencentcloudapi.com

## ByteDance VolcEngine ASR and TTS

# if you are using volcengine as ASR or TTS server, please set the following environment variables
# VOLCENGINE_APP_ID=volcengine_app_id
VOLCENGINE_ACCESS_TOKEN=volcengine_access_token

# You can choose different voice types and LLM models, please refer to the official documentation for more details:
# https://www.volcengine.com/docs/6561/1257544
# VOLCENGINE_VOICE_TYPE=zh_female_wanwanxiaohe_moon_bigtts

## ByteDance Doubao LLM
# if you are using volcengine as LLM server, please set the following environment variables
VOLCENGINE_DOUBAO_ACCESS_TOKEN=volcengine_doubao_access_token

# the default model is doubao-1-5-lite-16k-250115, you can also choose other models
# VOLCENGINE_DOUBAO_LLM_MODEL=doubao-1-5-lite-32k-250115

## Google Gemini
# if you are using google gemini as LLM server, please set the following environment variables
GOOGLE_GEMINI_API_KEY=your_api_key
# the default model is gemini-1.5-flash, you can also choose other models
# GOOGLE_GEMINI_MODEL=gemini-1.5-flash

## Ollama
# if you are using ollama as LLM server, please set the following environment variables
OLLAMA_ENDPOINT=http://localhost:11434
# the default model is deepseek-r1:1.5b, you can also choose other models, please refer to: https://ollama.com/library
# OLLAMA_MODEL=deepseek-r1:1.5b
# if you want to enable tools for ollama, uncomment the following line (make sure the model supports tools, otherwise it will return a 400 error):
# OLLAMA_ENABLE_TOOLS=true

## OpenAI
# if you are using openai as ASR、TTS or LLM server, please set the following environment variables
OPENAI_API_KEY=openai_api_key
# the default model is gpt-4o, you can also choose other models
# OPENAI_LLM_MODEL=gpt-4o
# if you are using other openai compatible api server, please set the following environment variables
# OPENAI_API_BASE_URL=https://api.openai.com/v2
# proxy settings for OpenAI， uncomment the following lines if you need to use a proxy to access OpenAI services
# HTTPS_PROXY=https://your_https_proxy
# HTTP_PROXY=http://your_http_proxy
# ALL_PROXY=socks5://your_socks_proxy

## Custom System Prompt
# you can set a custom system prompt for LLM, please uncomment the following line and set your own prompt
# SYSTEM_PROMPT=You are a happy girl and also a helpful assistant. Answer the question quickly in less than 5 sentences, also with a sense of humor.

